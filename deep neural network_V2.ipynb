{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_V2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Q-8rnDiiqdDuRIQO4cyJkHEbVWRsAdTi","authorship_tag":"ABX9TyPrBv7ye1DTW0dgjxR6itXB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wphNLxMSfHrd"},"source":["#import libraries\r\n","import numpy as np\r\n","# !pip install python-mnist\r\n","from mnist import MNIST\r\n","from sklearn.preprocessing import  LabelEncoder\r\n","from keras.utils import np_utils \r\n","import matplotlib.pyplot as plt  # import matplotlib for plotting and visualization\r\n","import matplotlib\r\n","%matplotlib inline\r\n","from sklearn.metrics import accuracy_score\r\n","import time\r\n","# libraries needed to convert notebook to html file\r\n","# !pip install jupyter\r\n","# !pip install nbconvert\r\n","import nbconvert\r\n","import jupyter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OPu3PaJ-dGs"},"source":["\r\n","class LinearLayer:\r\n","   \r\n","    def __init__(self, input_shape, n_out, ini_type=\"plain\"):\r\n","        \"\"\"\r\n","        The constructor of the LinearLayer takes the following parameters\r\n","        Args:\r\n","            input_shape: input shape of Data/Activations\r\n","            n_out: number of neurons in layer\r\n","            ini_type: initialization type for weight parameters, default is \"plain\"\r\n","        \"\"\"\r\n","\r\n","        self.m = input_shape[1]  # number of examples in training data\r\n","        # `params` store weights and bias in a python dictionary\r\n","        self.params = initialize_parameters(input_shape[0], n_out, ini_type)  # initialize weights and bias\r\n","        self.Z = np.zeros((self.params['W'].shape[0], input_shape[1]))  # create space for resultant Z output\r\n","\r\n","    def forward(self, A_prev):\r\n","        \"\"\"\r\n","        This function performs the forwards propagation using activations from previous layer\r\n","        Args:\r\n","            A_prev:  Activations/Input Data coming into the layer from previous layer\r\n","        \"\"\"\r\n","\r\n","        self.A_prev = A_prev  # store the Activations/Training Data coming in\r\n","        self.Z = np.dot(self.params['W'], self.A_prev) + self.params['b']  # compute the linear function\r\n","\r\n","    def backward(self, upstream_grad):\r\n","        \"\"\"\r\n","        This function performs the back propagation using upstream gradients\r\n","        Args:\r\n","            upstream_grad: gradient coming in from the upper layer to couple with local gradient\r\n","        \"\"\"\r\n","\r\n","        # derivative of Cost w.r.t W\r\n","        self.dW = np.dot(upstream_grad, self.A_prev.T)\r\n","\r\n","        # derivative of Cost w.r.t b, sum across rows\r\n","        self.db = np.sum(upstream_grad, axis=1, keepdims=True)\r\n","\r\n","        # derivative of Cost w.r.t A_prev\r\n","        self.dA_prev = np.dot(self.params['W'].T, upstream_grad)\r\n","\r\n","    def update_params(self, learning_rate=0.1):\r\n","        \"\"\"\r\n","        This function performs the gradient descent update\r\n","        Args:\r\n","            learning_rate: learning rate hyper-param for gradient descent, default 0.1\r\n","        \"\"\"\r\n","        self.params['W'] = self.params['W'] - learning_rate * self.dW  # update weights\r\n","        self.params['b'] = self.params['b'] - learning_rate * self.db  # update bias(es)\r\n","\r\n","\r\n","class SigmoidLayer:\r\n","    \"\"\"\r\n","    This file implements activation layers\r\n","    inline with a computational graph model\r\n","\r\n","    Args:\r\n","        shape: shape of input to the layer\r\n","\r\n","    Methods:\r\n","        forward(Z)\r\n","        backward(upstream_grad)\r\n","\r\n","    \"\"\"\r\n","\r\n","    def __init__(self, shape):\r\n","        \"\"\"\r\n","        The consturctor of the sigmoid/logistic activation layer takes in the following arguments\r\n","\r\n","        Args:\r\n","            shape: shape of input to the layer\r\n","        \"\"\"\r\n","        self.A = np.zeros(shape)  # create space for the resultant activations\r\n","\r\n","    def forward(self, Z):\r\n","        \"\"\"\r\n","        This function performs the forwards propagation step through the activation function\r\n","\r\n","        Args:\r\n","            Z: input from previous (linear) layer\r\n","        \"\"\"\r\n","        self.A = 1 / (1 + np.exp(-Z))  # compute activations\r\n","\r\n","    def backward(self, upstream_grad):\r\n","        \"\"\"\r\n","        This function performs the  back propagation step through the activation function\r\n","        Local gradient => derivative of sigmoid => A*(1-A)\r\n","        Args:\r\n","            upstream_grad: gradient coming into this layer from the layer above\r\n","        \"\"\"\r\n","        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\r\n","        self.dZ = upstream_grad * self.A*(1-self.A)\r\n","\r\n","\r\n","\r\n","class SoftmaxLayer:\r\n","    \"\"\"\r\n","    This file implements activation layers\r\n","    inline with a computational graph model\r\n","\r\n","    Args:\r\n","        shape: shape of input to the layer\r\n","\r\n","    Methods:\r\n","        forward(Z)\r\n","        backward(upstream_grad)\r\n","\r\n","    \"\"\"\r\n","\r\n","    def __init__(self, shape):\r\n","        \"\"\"\r\n","        The consturctor of the softmax activation layer takes in the following arguments\r\n","\r\n","        Args:\r\n","            shape: shape of input to the layer\r\n","        \"\"\"\r\n","        self.A = np.zeros(shape)  # create space for the resultant activations\r\n","        self.m = shape[1]\r\n","      \r\n","\r\n","    def forward(self, Z):\r\n","        \"\"\"\r\n","        This function performs the forwards propagation step through the activation function\r\n","        Args:\r\n","            Z: input from previous (linear) layer\r\n","        \"\"\"\r\n","        self.A = np.exp(Z) / np.sum(np.exp(Z),axis=0,keepdims=True)   # compute  activations\r\n","      \r\n","    def backward(self, upstream_grad,Y):\r\n","        \"\"\"\r\n","        This function performs the  back propagation step through the activation function\r\n","        Local gradient => derivative of sigmoid ==> (A-Y)\r\n","        Args:\r\n","            upstream_grad: gradient coming into this layer from the layer above\r\n","        \"\"\"\r\n","        # couple upstream gradient with local gradient, the result will be sent back to the Linear layer\r\n","        self.dZ = upstream_grad*( self.A - Y)/self.m  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQIIvXXfBjLs"},"source":["\r\n","def initialize_parameters(n_in, n_out, ini_type='plain'):\r\n","    \"\"\"\r\n","    Helper function to initialize some form of random weights and Zero biases\r\n","    Args:\r\n","        n_in: size of input layer\r\n","        n_out: size of output/number of neurons\r\n","        ini_type: set initialization type for weights\r\n","\r\n","    Returns:\r\n","        params: a dictionary containing W and b\r\n","    \"\"\"\r\n","\r\n","    params = dict()  # initialize empty dictionary of neural net parameters W and b\r\n","\r\n","    if ini_type == 'plain':\r\n","        params['W'] = np.random.randn(n_out, n_in) *0.01  # set weights 'W' to small random gaussian\r\n","    elif ini_type == 'xavier':\r\n","        params['W'] = np.random.randn(n_out, n_in) / (np.sqrt(n_in))  # set variance of W to 1/n\r\n","    elif ini_type == 'he':\r\n","        # Good when ReLU used in hidden layers\r\n","        # Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\r\n","        # Kaiming He et al. (https://arxiv.org/abs/1502.01852)\r\n","        # http: // cs231n.github.io / neural - networks - 2 /  # init\r\n","        params['W'] = np.random.randn(n_out, n_in) * np.sqrt(2/n_in)  # set variance of W to 2/n\r\n","\r\n","    params['b'] = np.zeros((n_out, 1))    # set bias 'b' to zeros\r\n","\r\n","    return params\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P31g1hOrtXrX","executionInfo":{"status":"ok","timestamp":1615257476612,"user_tz":-240,"elapsed":8149,"user":{"displayName":"James","photoUrl":"","userId":"12908900060583521650"}},"outputId":"bbb8d058-534c-47df-f8f8-8e99cffd46de"},"source":["mndata = MNIST('/content/drive/MyDrive/ML606/mnistData')\r\n","mndata.gz = True\r\n","train_images, train_labels = mndata.load_training()\r\n","test_images, test_labels = mndata.load_testing()\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: python-mnist in /usr/local/lib/python3.7/dist-packages (0.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"7LjVH-qTkdhb","executionInfo":{"status":"ok","timestamp":1615257480869,"user_tz":-240,"elapsed":12392,"user":{"displayName":"James","photoUrl":"","userId":"12908900060583521650"}},"outputId":"586a6fc0-0227-4e06-f2c8-9cc1eeeb7b73"},"source":["# preparing data sets for  Mini-Batch Gradient Descent\r\n","\r\n","Xtrain = np.array(train_images)/255.0     #normalizing train images to avoid overflow error and fast computation\r\n","Ytrain_labels = train_labels\r\n","Xtest= np.array(test_images) / 255.0    #normalizing test images to avoid overflow error and fast computation\r\n","Ytest_labels =  test_labels\r\n","n_batches =  60\r\n","\r\n","\r\n","\r\n","Xtrain.shape,len(Ytrain_labels), Xtest.shape,len(Ytest_labels)\r\n","\r\n","X_train_batches = np.split(Xtrain,n_batches)\r\n","Y_train_batches = np.split(np.array(Ytrain_labels), n_batches)\r\n","\r\n","# show random image\r\n","index = np.int(np.random.rand()*10)\r\n","# print(mndata.display(train_images[0]))\r\n","img = np.array(X_train_batches[0][index]).reshape(28,28)\r\n","plt.imshow(img)\r\n","plt.show()\r\n","print(Y_train_batches[0][index])\r\n","\r\n","print(\"the number of batches:\",(n_batches))\r\n","print(X_train_batches[0].shape,Y_train_batches[0].shape)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["0\n","the number of batches: 60\n","(1000, 784) (1000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ifdX3oirgUXl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615257480870,"user_tz":-240,"elapsed":12381,"user":{"displayName":"James","photoUrl":"","userId":"12908900060583521650"}},"outputId":"0f48e4c9-fa6d-47a3-f51e-d70dee78adb8"},"source":["\r\n","\r\n","def encode_Labels(Y):\r\n","  \"\"\"returns 10-bit hot encoded vectors of the labels\"\"\"\r\n","  feature_labels = np.array(Y) \r\n","  encoder = LabelEncoder() \r\n","  encoder.fit(feature_labels) \r\n","  feature_labels = encoder.transform(feature_labels) \r\n","  feature_labels = np_utils.to_categorical(feature_labels) \r\n","  return feature_labels\r\n","\r\n","\r\n","# hot encoding the labels  and transpose \r\n","Y_test = encode_Labels(Ytest_labels).T\r\n","\r\n","# transpose X_test images\r\n","X_test = Xtest.T\r\n","print(X_train_batches[0].T.shape)\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(784, 1000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h-gcP0ev-i94"},"source":["# define training constants\r\n","learning_rate = 1\r\n","number_of_epochs = 100\r\n","X_train_input_shape = X_train_batches[0].T.shape\r\n","Y_train_input_shape = (Y_test.shape[0],n_batches)\r\n","\r\n","\r\n","\r\n","np.random.seed(48) # set seed value so that the results are reproduceable\r\n","                  # (weights will now be initailzaed to the same pseudo-random numbers, each time)\r\n","\r\n","# Our network architecture has the shape: \r\n","#  (input)--> [Linear->Sigmoid]-->[Linear->Sigmoid]-->[Linear-->Sigmoid]-->[Linear-->Softmax] -->(output)  \r\n","\r\n","#------ LAYER-1 ----- define hidden layer that takes in training data \r\n","Z1 = LinearLayer(input_shape=X_train_input_shape, n_out=20, ini_type='xavier')\r\n","\r\n","A1 = SigmoidLayer(Z1.Z.shape)\r\n","\r\n","\r\n","# #------ LAYER-2 ----- define output layer that take is values from hidden layer\r\n","Z2= LinearLayer(input_shape=A1.A.shape, n_out=20, ini_type='xavier')\r\n","\r\n","# ----the last layer(Y_hat) ---activation is sigmoid \r\n","A2= SigmoidLayer(Z2.Z.shape)\r\n","\r\n","\r\n","# #------ LAYER-3 ----- define 3rd hidden layer that take is values from hidden layer\r\n","Z3= LinearLayer(input_shape=A2.A.shape, n_out=20, ini_type='xavier')\r\n","# ----the last layer(Y_hat) ---activation is sigmoid \r\n","A3= SigmoidLayer(Z3.Z.shape)\r\n","\r\n","\r\n","# #------ LAYER-4 ----- define output layer that take is values from hidden layer\r\n","Z4= LinearLayer(input_shape=A3.A.shape, n_out=10, ini_type='xavier')\r\n","# ----the last layer(Y_hat) ---activation is softmax \r\n","A4= SoftmaxLayer(Z4.Z.shape)\r\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JCTm4k0L-Ly"},"source":["def compute_cost(Y, Y_hat):\r\n","    \"\"\"\r\n","    This function computes and returns the Cost and its derivative.\r\n","    The is function uses the Squared Error Cost function -> (1/m)*sum(Y*log(Y_hat))\r\n","    Args:\r\n","        Y: labels of data\r\n","        Y_hat: Predictions(activations) from a last layer, the output layer\r\n","    Returns:\r\n","        cost: The Squared Error Cost result\r\n","        dY_hat: gradient of Cost w.r.t the Y_hat\r\n","    \"\"\"\r\n","    m = Y.shape[1]\r\n","\r\n","    cost = (-1 / m) * np.sum(np.multiply(Y,np.log(Y_hat)))\r\n","    cost = np.squeeze(cost)  # remove extraneous dimensions to give just a scalar\r\n","\r\n","    dY_hat = 1   # derivative of the  the cross-entropy with respect to Y_hat\r\n","\r\n","    return cost, dY_hat\r\n","\r\n","\r\n","def predict(X, Y, Zs, As):\r\n","    \"\"\"\r\n","    helper function to predict on data using a neural net model layers\r\n","    Args:\r\n","        X: Data in shape (features x num_of_examples)\r\n","        Y: labels in shape ( label x num_of_examples)\r\n","        Zs: All linear layers in form of a list e.g [Z1,Z2,...,Zn]\r\n","        As: All Activation layers in form of a list e.g [A1,A2,...,An]\r\n","    Returns::\r\n","        p: predicted labels\r\n","        probas : raw probabilities\r\n","        accuracy: the number of correct predictions from total predictions\r\n","    \"\"\"\r\n","    m = X.shape[1]\r\n","    n = len(Zs)  # number of layers in the neural network\r\n","    p = np.zeros((1, m))\r\n","\r\n","    # Forward propagation\r\n","    Zs[0].forward(X)\r\n","    As[0].forward(Zs[0].Z)\r\n","    for i in range(1, n):\r\n","        Zs[i].forward(As[i-1].A)\r\n","        As[i].forward(Zs[i].Z)\r\n","    probas = As[n-1].A\r\n","    return probas\r\n","\r\n","\r\n","def plot_learning_curve(costs, learning_rate, total_epochs, save=False):\r\n","    \"\"\"\r\n","    This function plots the Learning Curve of the model\r\n","    Args:\r\n","        costs: list of costs recorded during training\r\n","        learning_rate: the learning rate during training\r\n","        total_epochs: number of epochs the model was trained for\r\n","        save: bool flag to save the image or not. Default False\r\n","    \"\"\"\r\n","    # plot the cost\r\n","    plt.figure()\r\n","\r\n","    steps = int(total_epochs / len(costs))  # the steps at with costs were recorded\r\n","    plt.ylabel('Cost')\r\n","    plt.xlabel('Iterations ')\r\n","    plt.title(\"Learning rate =\" + str(learning_rate))\r\n","    plt.plot(np.squeeze(costs))\r\n","    locs, labels = plt.xticks()\r\n","    plt.xticks(locs[1:-1], tuple(np.array(locs[1:-1], dtype='int')*steps))  # change x labels of the plot\r\n","    plt.xticks()\r\n","    if save:\r\n","        plt.savefig('Cost_Curve.png', bbox_inches='tight')\r\n","    plt.show()\r\n"," \r\n","\r\n","\r\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P5j9v9efLpr3","executionInfo":{"status":"ok","timestamp":1615257545381,"user_tz":-240,"elapsed":76860,"user":{"displayName":"James","photoUrl":"","userId":"12908900060583521650"}},"outputId":"defbd9fc-be2b-4b07-975f-33e2bb58db03"},"source":["costs = [] # initially empty list, this will store all the costs after a certian number of epochs\r\n","\r\n","# Start training\r\n","start_time = time.time()\r\n","\r\n","for epoch in range(number_of_epochs):\r\n","  for batch in range(n_batches):\r\n","        # ------------------------- forward-prop -------------------------\r\n","       \r\n","        # # transpose Xtrain images\r\n","        X_train = X_train_batches[batch].T\r\n","        # #Hot encode Y_train and Transpose Y_train\r\n","        Y_train = encode_Labels(Y_train_batches[batch]).T\r\n","      \r\n","\r\n","      # --------------------forward propagation----------------------------\r\n","        Z1.forward(X_train)\r\n","        A1.forward(Z1.Z)\r\n","        \r\n","        Z2.forward(A1.A)\r\n","        A2.forward(Z2.Z)\r\n","\r\n","        Z3.forward(A2.A)\r\n","        A3.forward(Z3.Z)\r\n","\r\n","        Z4.forward(A3.A)\r\n","        A4.forward(Z4.Z)\r\n","\r\n","        # ---------------------- Compute Cost ----------------------------\r\n","        cost, dA4 = compute_cost(Y=Y_train, Y_hat=A4.A)\r\n","      \r\n","        \r\n","        # ------------------------- back-prop ----------------------------\r\n","        A4.backward(dA4,Y_train)\r\n","        Z4.backward(A4.dZ)\r\n","\r\n","        A3.backward(Z4.dA_prev)\r\n","        Z3.backward(A3.dZ)\r\n","        \r\n","        A2.backward(Z3.dA_prev)\r\n","        Z2.backward(A2.dZ)\r\n","        \r\n","        A1.backward(Z2.dA_prev)\r\n","        Z1.backward(A1.dZ)\r\n","        \r\n","        # ----------------------- Update weights and bias ----------------\r\n","        Z4.update_params(learning_rate=learning_rate)\r\n","        Z3.update_params(learning_rate=learning_rate)\r\n","        Z2.update_params(learning_rate=learning_rate)\r\n","        Z1.update_params(learning_rate=learning_rate)\r\n","        # # print and store Costs every 100 iterations.\r\n","  if (epoch % 20) == 0:\r\n","      #print(\"Cost at epoch#\" + str(epoch) + \": \" + str(cost))\r\n","      print(\"Cost at epoch = {}: Cost = {:.4f}\".format(epoch, cost))\r\n","  costs.append(cost)\r\n","print(\"Time elapsed to training the model = {:.3f} seconds\".format(time.time() - start_time))\r\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cost at epoch = 0: Cost = 2.2625\n","Cost at epoch = 20: Cost = 0.1684\n","Cost at epoch = 40: Cost = 0.1069\n","Cost at epoch = 60: Cost = 0.0866\n","Cost at epoch = 80: Cost = 0.0763\n","Time elapsed to training the model = 64.376 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OyFSAr7Qb-oL","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1615257545382,"user_tz":-240,"elapsed":76850,"user":{"displayName":"James","photoUrl":"","userId":"12908900060583521650"}},"outputId":"d0ac90b1-18b2-4cfd-c8bf-a14b7c0f684e"},"source":["\r\n","#plot the learning curve to see if our model error was getting minimized\r\n","\r\n","plot_learning_curve(costs,learning_rate,number_of_epochs)\r\n","\r\n","# get the true labels\r\n","Y_test_true = test_labels.tolist()\r\n","Y_train_true = Y_train_batches[n_batches-1].tolist()\r\n","\r\n","# see the ouptput predictions\r\n","train_predictions = predict(X=X_train, Y=Y_train, Zs=[Z1, Z2,Z3,Z4], As=[A1, A2,A3,A4])\r\n","test_predictions =  predict(X=X_test, Y=Y_test, Zs=[Z1, Z2,Z3,Z4], As=[A1, A2,A3,A4])\r\n","\r\n","Y_train_pred = train_predictions.argmax(axis=0)\r\n","Y_test_pred = test_predictions.argmax(axis=0)\r\n","\r\n","\r\n","training_accuracy = accuracy_score(Y_train_true,Y_train_pred)\r\n","testing_accuracy = accuracy_score(Y_test_true,Y_test_pred)\r\n","\r\n","print(\"\\n\\nThe  accuracy of the model on the last batch of training data sets is: {:.2f}%\".format(training_accuracy*100), end=\"\\n\")\r\n","print(\"The accuracy of the model on testing data sets is: {:.2f}%\".format(testing_accuracy*100))\r\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n","\n","The  accuracy of the model on the last batch of training data sets is: 98.90%\n","The accuracy of the model on testing data sets is: 95.53%\n"],"name":"stdout"}]}]}